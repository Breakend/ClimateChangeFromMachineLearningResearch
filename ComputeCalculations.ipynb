{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoph and Lee (2016): 4.00 PFLOPS-day | 5265 Kwatt/hour | 4012 GCPWCC | 4353 EPAWCC | 2714 US AVE CO2\n",
      "Zoph et al (2017): 0.77 PFLOPS-day | 499 Kwatt/hour | 380 GCPWCC | 413 EPAWCC | 257 US AVE CO2\n",
      "Zhong et al. (2018): 0.34 PFLOPS-day | 179 Kwatt/hour | 136 GCPWCC | 148 EPAWCC | 92 US AVE CO2\n",
      "Elsken et al. (2018): 0.20 PFLOPS-day | 116 Kwatt/hour | 88 GCPWCC | 96 EPAWCC | 60 US AVE CO2\n",
      "OpenGo et al. (2018): 85.93 PFLOPS-day | 55440 Kwatt/hour | 42245 GCPWCC | 45834 EPAWCC | 28580 US AVE CO2\n",
      "Chen et al. (2018): 7.95 PFLOPS-day | 5128 Kwatt/hour | 3907 GCPWCC | 4239 EPAWCC | 2643 US AVE CO2\n",
      "OpenAI 1v1 Dota (2018): 13.23 PFLOPS-day | 10903 Kwatt/hour | 8308 GCPWCC | 9014 EPAWCC | 5620 US AVE CO2\n",
      "OpenAI Five Dota (2018): 14.14 PFLOPS-day | 9123 Kwatt/hour | 6952 GCPWCC | 7543 EPAWCC | 4703 US AVE CO2\n",
      "AlphaGo (2016): 3.25 PFLOPS-day | 2860 Kwatt/hour | 2179 GCPWCC | 2364 EPAWCC | 1474 US AVE CO2\n",
      "Wu et al. (2018): 1.66 PFLOPS-day | 1368 Kwatt/hour | 1042 GCPWCC | 1131 EPAWCC | 705 US AVE CO2\n",
      "OpenAI 5 (Aug 5 model): 190 PFLOPS-day 122580 Kwatt/hour 93406.4516129 (101342.25624) GCP CO2 (EPA CO2 ) ; 63192.886448 US AVE CO2\n",
      "Google Translate Daily: 1915100.0 PFLOPS-day 37459168 Kwatt/hour 28543886.5526 (30968972.8616) GCP CO2 (EPA CO2 ) ; 19310984.954 US AVE CO2\n"
     ]
    }
   ],
   "source": [
    "# Calculations for compute based on https://blog.openai.com/ai-and-compute/\n",
    "# Method 2: #Total GPU Hours * #GPU-GFLOPS * .33 utilization\n",
    "\n",
    "UTILIZATION_CONSTANT = .33\n",
    "KG_PER_LB=0.453592\n",
    "EPA_WORST_CASE_CO2_per_kilowatthr = 1822.65*KG_PER_LB/1000.0\n",
    "GCP_WORST_CASE_CO2_per_kilowatthr = 762./1000.0\n",
    "US_AV_CO2_PER_kilowatthr = 1136.53*KG_PER_LB/1000.0\n",
    "\n",
    "def print_stats(name, gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True):\n",
    "\n",
    "    pflop_hours = pflops_per_gpu*gpu_hours # \n",
    "    util_const = 1.0 if not utilization_constant_use else UTILIZATION_CONSTANT\n",
    "    utilized_pflop_hours = pflop_hours *util_const\n",
    "    utilized_pflops_day = utilized_pflop_hours / 24.\n",
    "    utilized_gflops_day = utilized_pflops_day * (1000000 ) # gigaflops/1 petaflop\n",
    "    watt_day = utilized_gflops_day / gflop_watts\n",
    "    kwatt_day = watt_day / 1000.\n",
    "    kwatt_hour = kwatt_day * 24 # hours/day \n",
    "    GCP_emissions = kwatt_hour * GCP_WORST_CASE_CO2_per_kilowatthr\n",
    "    EPA_emissions = kwatt_hour * EPA_WORST_CASE_CO2_per_kilowatthr\n",
    "    AVE_EMISSONS = kwatt_hour * US_AV_CO2_PER_kilowatthr\n",
    "    print(\"{}: {:.2f} PFLOPS-day | {} Kwatt/hour | {} GCPWCC | {} EPAWCC | {} US AVE CO2\".format(name, utilized_pflops_day, int(kwatt_hour), int(GCP_emissions), int(EPA_emissions), int(AVE_EMISSONS)))\n",
    "\n",
    "# Zoph and Lee (2016)\n",
    "# 800 GPUs for 28 days resulting in 22,400 GPU-hours. Nvidia K40 GPUs\n",
    "gpu_hours = 22400  #800 * 28 * 24\n",
    "pflops_per_gpu = 0.00429 #4.29 Tflops for k40 https://www.nvidia.com/content/tesla/pdf/nvidia-tesla-kepler-family-datasheet.pdf\n",
    "gflop_watts = 18.25 # k40\n",
    "print_stats(\"Zoph and Lee (2016)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=False)\n",
    "\n",
    "# Zoph et al. (2018)\n",
    "# 500 GPUs across 4 days resulting in 2,000 GPU-hours. NVidia P100s.\n",
    "gpu_hours = 2000 #500 * 4 * 24\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "print_stats(\"Zoph et al (2017)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=False)\n",
    "\n",
    "# Zhong et al. (2018)\n",
    "# only spends\n",
    "# 3\n",
    "# days with\n",
    "# 32\n",
    "# GPUs,\n",
    "gpu_hours = 32 * 3 * 24\n",
    "pflops_per_gpu = 0.0106 # 1080ti\n",
    "gflop_watts = 45 # 1080ti\n",
    "print_stats(\"Zhong et al. (2018)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "\n",
    "\n",
    "## Elsken et al.2018\n",
    "# https://arxiv.org/pdf/1804.09081.pdf#page=14&zoom=auto,-139,431\n",
    "# 56 GPU days \n",
    "# Titan X\n",
    "gpu_hours = 56 * 24\n",
    "pflops_per_gpu = 0.01079 # x\n",
    "gflop_watts = 41 # x\n",
    "print_stats(\"Elsken et al. (2018)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "## Elf opengo et al.2018\n",
    "# 2000 GPUs for \n",
    "# assuming p100, not sure\n",
    "gpu_hours = 2000 * 14 * 24\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "print_stats(\"OpenGo et al. (2018)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "\n",
    "## Chen et al.\n",
    "# 370 for 1 week\n",
    "# assume p100\n",
    "gpu_hours = 370 * 7 * 24\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "print_stats(\"Chen et al. (2018)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "## OpenAI 1v1 Dota\n",
    "# 256 K80 GPUs for 18 days (rough estimate from the charts provided)\n",
    "gpu_hours = 256 * 18 * 24\n",
    "pflops_per_gpu = 0.0087 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 29.12 # p100\n",
    "print_stats(\"OpenAI 1v1 Dota (2018)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "## OpenAI Five Dota\n",
    "# 256 P100 GPUs for 18 days (taken from the charts provided)\n",
    "gpu_hours = 256 * 18 * 24\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "print_stats(\"OpenAI Five Dota (2018)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "# AlphaGo https://www.nature.com/articles/nature16961\n",
    "# Policy network: classification  50 GPUs around 3 weeks\n",
    "# Policy network: reinforcement learning 50 GPUs, for one day.\n",
    "# Value network: regression 50 GPUs, for one week.\n",
    "# Not able to find GPU information, assuming training wan run on M40 (November 10, 2015) so we can get a rough estimate\n",
    "gpu_hours = 50 * 3 * 7 * 24 + 50 * 24 + 50 * 7 * 24\n",
    "pflops_per_gpu = 0.0068 # M40 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 27.3 # M40\n",
    "print_stats(\"AlphaGo (2016)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "## Google Translate Training\n",
    "## MT En â†’ Fr, it takes around 6 days to train a basic model using 96 NVIDIA K80 GPUs\n",
    "gpu_hours = 96 * 6 * 24\n",
    "pflops_per_gpu = 0.008736 # k80 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 29.12# k80\n",
    "print_stats(\"Wu et al. (2018)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "# OpenAI 1v1 https://blog.openai.com/openai-five-benchmark-results/\n",
    "\n",
    "# pflops_per_gpu = 0.008736 # k80 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "# gflop_watts = 29.12# k80\n",
    "# utilized_pflops_day = 8\n",
    "# utilized_gflops_day = utilized_pflops_day * 1000000.\n",
    "# watt_day = utilized_gflops_day / gflop_watts\n",
    "# kwatt_day = watt_day / 1000.\n",
    "# kwatt_hour = kwatt_day * 24 # hours/day \n",
    "# GCP_emissions = kwatt_hour * GCP_WORST_CASE_CO2_per_kilowatthr\n",
    "# EPA_emissions = kwatt_hour * EPA_WORST_CASE_CO2_per_kilowatthr\n",
    "# AVE_EMISSONS = kwatt_hour * US_AV_CO2_PER_kilowatthr\n",
    "# name = \"OpenAI 1v1\"\n",
    "# print(\"{}: {} PFLOPS-day {} Kwatt/hour {} ({}) GCP CO2 (EPA CO2 ) ; {} US AVE CO2\".format(name, utilized_pflops_day, int(kwatt_hour), GCP_emissions, EPA_emissions, AVE_EMISSONS))\n",
    "     \n",
    "# # OpenAI 5 aug 5 https://blog.openai.com/openai-five-benchmark-results/\n",
    "# pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "# gflop_watts = 37.2 # p100\n",
    "# utilized_pflops_day = 40\n",
    "# utilized_gflops_day = utilized_pflops_day * 1000000.\n",
    "# watt_day = utilized_gflops_day / gflop_watts\n",
    "# kwatt_day = watt_day / 1000.\n",
    "# kwatt_hour = kwatt_day * 24 # hours/day \n",
    "# GCP_emissions = kwatt_hour * GCP_WORST_CASE_CO2_per_kilowatthr\n",
    "# EPA_emissions = kwatt_hour * EPA_WORST_CASE_CO2_per_kilowatthr\n",
    "# AVE_EMISSONS = kwatt_hour * US_AV_CO2_PER_kilowatthr\n",
    "# name = \"OpenAI 5 (June 6 model)\"\n",
    "# print(\"{}: {} PFLOPS-day {} Kwatt/hour {} ({}) GCP CO2 (EPA CO2 ) ; {} US AVE CO2\".format(name, utilized_pflops_day, int(kwatt_hour), GCP_emissions, EPA_emissions, AVE_EMISSONS))\n",
    "            \n",
    "\n",
    "# OpenAI 5 aug 5 https://blog.openai.com/openai-five-benchmark-results/\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "utilized_pflops_day = 190\n",
    "utilized_gflops_day = utilized_pflops_day * 1000000.\n",
    "watt_day = utilized_gflops_day / gflop_watts\n",
    "kwatt_day = watt_day / 1000.\n",
    "kwatt_hour = kwatt_day * 24 # hours/day \n",
    "GCP_emissions = kwatt_hour * GCP_WORST_CASE_CO2_per_kilowatthr\n",
    "EPA_emissions = kwatt_hour * EPA_WORST_CASE_CO2_per_kilowatthr\n",
    "AVE_EMISSONS = kwatt_hour * US_AV_CO2_PER_kilowatthr\n",
    "name = \"OpenAI 5 (Aug 5 model)\"\n",
    "print(\"{}: {} PFLOPS-day {} Kwatt/hour {} ({}) GCP CO2 (EPA CO2 ) ; {} US AVE CO2\".format(name, utilized_pflops_day, int(kwatt_hour), GCP_emissions, EPA_emissions, AVE_EMISSONS))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "pflops_per_gpu = .092 # tpu\n",
    "gflop_watts = 1227.# tpu\n",
    "utilized_pflops_day = 1.9151 * 10**6 \n",
    "utilized_gflops_day = utilized_pflops_day * 1000000.\n",
    "watt_day = utilized_gflops_day / gflop_watts\n",
    "kwatt_day = watt_day / 1000.\n",
    "kwatt_hour = kwatt_day * 24 # hours/day \n",
    "GCP_emissions = kwatt_hour * GCP_WORST_CASE_CO2_per_kilowatthr\n",
    "EPA_emissions = kwatt_hour * EPA_WORST_CASE_CO2_per_kilowatthr\n",
    "AVE_EMISSONS = kwatt_hour * US_AV_CO2_PER_kilowatthr\n",
    "name = \"Google Translate Daily\"\n",
    "print(\"{}: {} PFLOPS-day {} Kwatt/hour {} ({}) GCP CO2 (EPA CO2 ) ; {} US AVE CO2\".format(name, utilized_pflops_day, int(kwatt_hour), GCP_emissions, EPA_emissions, AVE_EMISSONS))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
