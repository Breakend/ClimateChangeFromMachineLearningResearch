{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoph and Lee (2016): 31.71 PFLOPS-day | 41690 Kwatt-hour | 34467 EPAWCC | 21492 US AVE CO2\n",
      " (So et al.,2019): 41.27 PFLOPS-day | 2691 Kwatt-hour | 2225 EPAWCC | 1387 US AVE CO2\n",
      " (So et al.,2019 using Strubell 2019 assumptions: 280.42 PFLOPS-day | 180919 Kwatt-hour | 149573 EPAWCC | 93267 US AVE CO2\n",
      "Zoph et al (2017): 6.14 PFLOPS-day | 3960 Kwatt-hour | 3273 EPAWCC | 2041 US AVE CO2\n",
      "Zhong et al. (2018): 0.34 PFLOPS-day | 190 Kwatt-hour | 157 EPAWCC | 97 US AVE CO2\n",
      "Elsken et al. (2018): 0.20 PFLOPS-day | 110 Kwatt-hour | 91 EPAWCC | 57 US AVE CO2\n",
      "OpenGo et al. (2018): 85.93 PFLOPS-day | 55440 Kwatt-hour | 45834 EPAWCC | 28580 US AVE CO2\n",
      "Chen et al. (2018): 7.95 PFLOPS-day | 5128 Kwatt-hour | 4239 EPAWCC | 2643 US AVE CO2\n",
      "OpenAI 1v1 Dota (2018): 13.23 PFLOPS-day | 9123 Kwatt-hour | 7543 EPAWCC | 4703 US AVE CO2\n",
      "OpenAI Five Dota (2018): 14.14 PFLOPS-day | 9123 Kwatt-hour | 7543 EPAWCC | 4703 US AVE CO2\n",
      "AlphaGo (2016): 3.25 PFLOPS-day | 2871 Kwatt-hour | 2373 EPAWCC | 1480 US AVE CO2\n",
      "AlphaGo Zero (2018): 118.27 PFLOPS-day | 5068 Kwatt-hour | 4190 EPAWCC | 2613 US AVE CO2\n",
      "Wu et al. (2018): 1.66 PFLOPS-day | 1368 Kwatt-hour | 1131 EPAWCC | 705 US AVE CO2\n",
      "OpenAI 5 (Aug 5 model): 190 PFLOPS-day 122580 Kwatt/hour 93406.45161290321 (101342.25624) GCP CO2 (EPA CO2 ) ; 63192.886448 US AVE CO2\n",
      "Google Translate Daily: 1915100.0 PFLOPS-day 37459168 Kwatt/hour 28543886.55256724 (30968972.86157223) GCP CO2 (EPA CO2 ) ; 19310984.95397508 US AVE CO2\n"
     ]
    }
   ],
   "source": [
    "# Calculations for compute based on https://blog.openai.com/ai-and-compute/\n",
    "# Method 2: #Total GPU Hours * #GPU-GFLOPS * .33 utilization\n",
    "\n",
    "UTILIZATION_CONSTANT = 0.33\n",
    "KG_PER_LB=0.453592\n",
    "EPA_WORST_CASE_CO2_per_kilowatthr = 1822.65*KG_PER_LB/1000.0\n",
    "GCP_WORST_CASE_CO2_per_kilowatthr = 762./1000.0\n",
    "US_AV_CO2_PER_kilowatthr = 1136.53*KG_PER_LB/1000.0\n",
    "\n",
    "# def print_stats(name, gpu_hours, pflops_per_gpu_sec, gflop_watts, utilization_constant_use=True):\n",
    "#     pflops_per_gpu_hr = pflops_per_gpu_sec * 3600. #pflop/h\n",
    "#     pflop_hours = pflops_per_gpu_hr * gpu_hours #pflop (floating point operations performed) \n",
    "#     util_const = 1.0 if not utilization_constant_use else UTILIZATION_CONSTANT \n",
    "#     utilized_pflop_hours = pflop_hours / util_const # we assume it's less than fully utilized the whole time\n",
    "#     utilized_pflops_day = utilized_pflop_hours / 24.\n",
    "#     utilized_gflops_day = utilized_pflops_day * (1000000 ) # gigaflops/1 petaflop\n",
    "#     watt_day = utilized_gflops_day / gflop_watts\n",
    "#     kwatt_day = watt_day / 1000.\n",
    "#     kwatt_hour = kwatt_day * 24 # hours/day \n",
    "#     GCP_emissions = kwatt_hour * GCP_WORST_CASE_CO2_per_kilowatthr\n",
    "#     EPA_emissions = kwatt_hour * EPA_WORST_CASE_CO2_per_kilowatthr\n",
    "#     AVE_EMISSONS = kwatt_hour * US_AV_CO2_PER_kilowatthr\n",
    "#     print(\"\")\n",
    "#     print(\"{}: {:.2f} PFLOPS-day | {} Kwatt/hour | {} GCPWCC | {} EPAWCC | {} US AVE CO2\".format(name, utilized_pflops_day, int(kwatt_hour), int(GCP_emissions), int(EPA_emissions), int(AVE_EMISSONS)))\n",
    "\n",
    "def print_stats(name, gpu_hours, tdp, gpu_pflop_per_sec):\n",
    "    # a pflops day is peta floating point operation per second for one whole day (same as a kw-hr)\n",
    "    utilized_pflops_day = gpu_hours / 24 * gpu_pflop_per_sec  * UTILIZATION_CONSTANT\n",
    "    # can calculate kwatt_hours by taking gpu_hours * max wattage (W-hr) * util constant / 1000.0 (to get kw) \n",
    "    kwatt_hour = gpu_hours * tdp / 1000. * UTILIZATION_CONSTANT\n",
    "    GCP_emissions = kwatt_hour * GCP_WORST_CASE_CO2_per_kilowatthr\n",
    "    EPA_emissions = kwatt_hour * EPA_WORST_CASE_CO2_per_kilowatthr\n",
    "    AVE_EMISSONS = kwatt_hour * US_AV_CO2_PER_kilowatthr\n",
    "    print(\"{}: {:.2f} PFLOPS-day | {} Kwatt-hour | {} EPAWCC | {} US AVE CO2\".format(name, utilized_pflops_day, int(kwatt_hour), int(EPA_emissions), int(AVE_EMISSONS)))\n",
    "\n",
    "# Zoph and Lee (2016)\n",
    "# 800 GPUs for 28 days resulting in 22,400 GPU-hours. Nvidia K40 GPUs\n",
    "gpu_hours = 800 * 28 * 24\n",
    "pflops_per_gpu = 0.00429 #4.29 Tflops for k40 https://www.nvidia.com/content/tesla/pdf/nvidia-tesla-kepler-family-datasheet.pdf\n",
    "gflop_watts = 18.25 # k40\n",
    "k40_tdp = 235\n",
    "print_stats(\"Zoph and Lee (2016)\", gpu_hours, k40_tdp, pflops_per_gpu)\n",
    "\n",
    "\n",
    "# NAS from strubell (2016)\n",
    "gpu_hours = 32623 # 10 hours on tpu v2 #* 8 \n",
    "# pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "# gflop_watts = 37.2 # tpu\n",
    "pflops_per_gpu = .092 # tpu\n",
    "gflop_watts = 1227.# tpu\n",
    "# 274,120 hours on 8 P100 GPUs\n",
    "print_stats(\" (So et al.,2019)\", gpu_hours, 250, pflops_per_gpu)\n",
    "\n",
    "gpu_hours = 274120 * 8 # 10 hours on tpu v2 #* 8 \n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # tpu\n",
    "# pflops_per_gpu = .092 # tpu\n",
    "# gflop_watts = 1227.# tpu\n",
    "# 274,120 hours on 8 P100 GPUs\n",
    "print_stats(\" (So et al.,2019 using Strubell 2019 assumptions\", gpu_hours, 250, pflops_per_gpu)\n",
    "\n",
    "\n",
    "# Zoph et al. (2018)\n",
    "# 500 GPUs across 4 days resulting in 2,000 GPU-hours. NVidia P100s.\n",
    "gpu_hours = 500 * 4 * 24\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "print_stats(\"Zoph et al (2017)\", gpu_hours, 250, pflops_per_gpu) #gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=False)\n",
    "\n",
    "# Zhong et al. (2018)\n",
    "# only spends\n",
    "# 3\n",
    "# days with\n",
    "# 32\n",
    "# GPUs,\n",
    "gpu_hours = 32 * 3 * 24\n",
    "pflops_per_gpu = 0.0106 # 1080ti\n",
    "gflop_watts = 45 # 1080ti\n",
    "print_stats(\"Zhong et al. (2018)\", gpu_hours, 250, pflops_per_gpu)#pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "\n",
    "\n",
    "## Elsken et al.2018\n",
    "# https://arxiv.org/pdf/1804.09081.pdf#page=14&zoom=auto,-139,431\n",
    "# 56 GPU days \n",
    "# Titan X\n",
    "gpu_hours = 56 * 24\n",
    "pflops_per_gpu = 0.01079 # x\n",
    "gflop_watts = 41 # x\n",
    "print_stats(\"Elsken et al. (2018)\", gpu_hours, 250, pflops_per_gpu)#pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "## Elf opengo et al.2018\n",
    "# 2000 GPUs for \n",
    "# assuming p100, not sure\n",
    "gpu_hours = 2000 * 14 * 24\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "print_stats(\"OpenGo et al. (2018)\", gpu_hours, 250, pflops_per_gpu)#pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "\n",
    "## Chen et al.\n",
    "# 370 for 1 week\n",
    "# assume p100\n",
    "gpu_hours = 370 * 7 * 24\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "print_stats(\"Chen et al. (2018)\", gpu_hours, 250, pflops_per_gpu) #pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "## OpenAI 1v1 Dota\n",
    "# 256 K80 GPUs for 18 days (rough estimate from the charts provided)\n",
    "gpu_hours = 256 * 18 * 24\n",
    "pflops_per_gpu = 0.0087 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 29.12 # p100\n",
    "print_stats(\"OpenAI 1v1 Dota (2018)\", gpu_hours, 250, pflops_per_gpu)#pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "## OpenAI Five Dota\n",
    "# 256 P100 GPUs for 18 days (taken from the charts provided)\n",
    "gpu_hours = 256 * 18 * 24\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "print_stats(\"OpenAI Five Dota (2018)\", gpu_hours, 250, pflops_per_gpu)#pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "# AlphaGo https://www.nature.com/articles/nature16961\n",
    "# Policy network: classification  50 GPUs around 3 weeks\n",
    "# Policy network: reinforcement learning 50 GPUs, for one day.\n",
    "# Value network: regression 50 GPUs, for one week.\n",
    "# Not able to find GPU information, assuming training were run on M40 (released on November 10, 2015) so we can get a rough estimate\n",
    "gpu_hours = 50 * 3 * 7 * 24 + 50 * 24 + 50 * 7 * 24\n",
    "pflops_per_gpu = 0.0068 # M40 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 27.3 # M40\n",
    "print_stats(\"AlphaGo (2016)\", gpu_hours, 250, pflops_per_gpu)#pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "# AlphaZero Mastering the Game of Go without Human Knowledge\n",
    "# Training runs for approximately 40 days, * 64 GPU workers\n",
    "# Not able to find GPU information, paper published on Oct. 18, 2017, assuming experiments run on NVIDIA V100 PCIe, which is the latest model available then.\n",
    "gpu_hours = 40 * 24 * 64\n",
    "pflops_per_gpu = 0.14 # NVIDIA V100 PCIe\n",
    "gflop_watts = 56\n",
    "print_stats(\"AlphaGo Zero (2018)\", gpu_hours, 250, pflops_per_gpu)#pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "## Google Translate Training\n",
    "## MT En â†’ Fr, it takes around 6 days to train a basic model using 96 NVIDIA K80 GPUs\n",
    "gpu_hours = 96 * 6 * 24\n",
    "pflops_per_gpu = 0.008736 # k80 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 29.12# k80\n",
    "print_stats(\"Wu et al. (2018)\", gpu_hours, 300, pflops_per_gpu)# pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "# OpenAI 1v1 https://blog.openai.com/openai-five-benchmark-results/\n",
    "\n",
    "# pflops_per_gpu = 0.008736 # k80 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "# gflop_watts = 29.12# k80\n",
    "# utilized_pflops_day = 8\n",
    "# utilized_gflops_day = utilized_pflops_day * 1000000.\n",
    "# watt_day = utilized_gflops_day / gflop_watts\n",
    "# kwatt_day = watt_day / 1000.\n",
    "# kwatt_hour = kwatt_day * 24 # hours/day \n",
    "# GCP_emissions = kwatt_hour * GCP_WORST_CASE_CO2_per_kilowatthr\n",
    "# EPA_emissions = kwatt_hour * EPA_WORST_CASE_CO2_per_kilowatthr\n",
    "# AVE_EMISSONS = kwatt_hour * US_AV_CO2_PER_kilowatthr\n",
    "# name = \"OpenAI 1v1\"\n",
    "# print(\"{}: {} PFLOPS-day {} Kwatt/hour {} ({}) GCP CO2 (EPA CO2 ) ; {} US AVE CO2\".format(name, utilized_pflops_day, int(kwatt_hour), GCP_emissions, EPA_emissions, AVE_EMISSONS))\n",
    "     \n",
    "# # OpenAI 5 aug 5 https://blog.openai.com/openai-five-benchmark-results/\n",
    "# pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "# gflop_watts = 37.2 # p100\n",
    "# utilized_pflops_day = 40\n",
    "# utilized_gflops_day = utilized_pflops_day * 1000000.\n",
    "# watt_day = utilized_gflops_day / gflop_watts\n",
    "# kwatt_day = watt_day / 1000.\n",
    "# kwatt_hour = kwatt_day * 24 # hours/day \n",
    "# GCP_emissions = kwatt_hour * GCP_WORST_CASE_CO2_per_kilowatthr\n",
    "# EPA_emissions = kwatt_hour * EPA_WORST_CASE_CO2_per_kilowatthr\n",
    "# AVE_EMISSONS = kwatt_hour * US_AV_CO2_PER_kilowatthr\n",
    "# name = \"OpenAI 5 (June 6 model)\"\n",
    "# print(\"{}: {} PFLOPS-day {} Kwatt/hour {} ({}) GCP CO2 (EPA CO2 ) ; {} US AVE CO2\".format(name, utilized_pflops_day, int(kwatt_hour), GCP_emissions, EPA_emissions, AVE_EMISSONS))\n",
    "            \n",
    "\n",
    "# OpenAI 5 aug 5 https://blog.openai.com/openai-five-benchmark-results/\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "utilized_pflops_day = 190\n",
    "utilized_gflops_day = utilized_pflops_day * 1000000.\n",
    "watt_day = utilized_gflops_day / gflop_watts\n",
    "kwatt_day = watt_day / 1000.\n",
    "kwatt_hour = kwatt_day * 24 # hours/day \n",
    "GCP_emissions = kwatt_hour * GCP_WORST_CASE_CO2_per_kilowatthr\n",
    "EPA_emissions = kwatt_hour * EPA_WORST_CASE_CO2_per_kilowatthr\n",
    "AVE_EMISSONS = kwatt_hour * US_AV_CO2_PER_kilowatthr\n",
    "name = \"OpenAI 5 (Aug 5 model)\"\n",
    "print(\"{}: {} PFLOPS-day {} Kwatt/hour {} ({}) GCP CO2 (EPA CO2 ) ; {} US AVE CO2\".format(name, utilized_pflops_day, int(kwatt_hour), GCP_emissions, EPA_emissions, AVE_EMISSONS))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "pflops_per_gpu = .092 # tpu\n",
    "gflop_watts = 1227.# tpu\n",
    "utilized_pflops_day = 1.9151 * 10**6 \n",
    "utilized_gflops_day = utilized_pflops_day * 1000000.\n",
    "watt_day = utilized_gflops_day / gflop_watts\n",
    "kwatt_day = watt_day / 1000.\n",
    "kwatt_hour = kwatt_day * 24 # hours/day \n",
    "GCP_emissions = kwatt_hour * GCP_WORST_CASE_CO2_per_kilowatthr\n",
    "EPA_emissions = kwatt_hour * EPA_WORST_CASE_CO2_per_kilowatthr\n",
    "AVE_EMISSONS = kwatt_hour * US_AV_CO2_PER_kilowatthr\n",
    "name = \"Google Translate Daily\"\n",
    "print(\"{}: {} PFLOPS-day {} Kwatt/hour {} ({}) GCP CO2 (EPA CO2 ) ; {} US AVE CO2\".format(name, utilized_pflops_day, int(kwatt_hour), GCP_emissions, EPA_emissions, AVE_EMISSONS))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
