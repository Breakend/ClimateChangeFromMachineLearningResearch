{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoph and Lee (2016): 4.0040000000000004 PFLOPS-day 5265.534246575344 Kwatt/hour 4012.337095890412 (9597.225994520551) EPA CO2 (GCP CO2 ) ; 5984.437637260276 US AVE CO2\n",
      "Zoph et al (2017): 0.7749999999999999 PFLOPS-day 499.9999999999999 Kwatt/hour 380.99999999999994 (911.3249999999998) EPA CO2 (GCP CO2 ) ; 568.2649999999999 US AVE CO2\n",
      "Zhong et al. (2018): 0.33580800000000005 PFLOPS-day 179.09760000000003 Kwatt/hour 136.47237120000003 (326.4322406400001) EPA CO2 (GCP CO2 ) ; 203.54979532800004 US AVE CO2\n",
      "Elsken et al. (2018): 0.1993992 PFLOPS-day 116.72148292682928 Kwatt/hour 88.94176999024391 (212.7424108565854) EPA CO2 (GCP CO2 ) ; 132.65746699082928 US AVE CO2\n",
      "OpenGo et al. (2018): 85.932 PFLOPS-day 55440.0 Kwatt/hour 42245.28 (101047.716) EPA CO2 (GCP CO2 ) ; 63009.2232 US AVE CO2\n",
      "Chen et al. (2018): 7.948709999999999 PFLOPS-day 5128.2 Kwatt/hour 3907.6884 (9346.91373) EPA CO2 (GCP CO2 ) ; 5828.353146 US AVE CO2\n",
      "OpenAI 1v1 Dota (2018): 0.551232 PFLOPS-day 454.3120879120879 Kwatt/hour 346.185810989011 (828.0519270329671) EPA CO2 (GCP CO2 ) ; 516.3393172747253 US AVE CO2\n"
     ]
    }
   ],
   "source": [
    "# Calculations for compute based on https://blog.openai.com/ai-and-compute/\n",
    "# Method 2: #Total GPU Hours * #GPU-GFLOPS * .33 utilization\n",
    "\n",
    "UTILIZATION_CONSTANT = .33\n",
    "EPA_WORST_CASE_CO2_per_kilowatthr = 1822.65/1000.0\n",
    "GCP_WORST_CASE_CO2_per_kilowatthr = 762/1000.0\n",
    "US_AV_CO2_PER_kilowatthr = 1136.53/1000.0\n",
    "\n",
    "def print_stats(name, gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True):\n",
    "\n",
    "    pflop_hours = pflops_per_gpu*gpu_hours # \n",
    "    util_const = 1.0 if not utilization_constant_use else UTILIZATION_CONSTANT\n",
    "    utilized_pflop_hours = pflop_hours *util_const\n",
    "    utilized_pflops_day = utilized_pflop_hours / 24.\n",
    "    utilized_gflops_day = utilized_pflops_day * (1000000 ) # gigaflops/1 petaflop\n",
    "    watt_day = utilized_gflops_day / gflop_watts\n",
    "    kwatt_day = watt_day / 1000.\n",
    "    kwatt_hour = kwatt_day * 24 # hours/day \n",
    "    GCP_emissions = kwatt_hour * GCP_WORST_CASE_CO2_per_kilowatthr\n",
    "    EPA_emissions = kwatt_hour * EPA_WORST_CASE_CO2_per_kilowatthr\n",
    "    AVE_EMISSONS = kwatt_hour * US_AV_CO2_PER_kilowatthr\n",
    "    print(\"{}: {} PFLOPS-day {} Kwatt/hour {} ({}) EPA CO2 (GCP CO2 ) ; {} US AVE CO2\".format(name, utilized_pflops_day, kwatt_hour, GCP_emissions, EPA_emissions, AVE_EMISSONS))\n",
    "\n",
    "# Zoph and Lee (2016)\n",
    "# 800 GPUs for 28 days resulting in 22,400 GPU-hours. Nvidia K40 GPUs\n",
    "gpu_hours = 22400  #800 * 28 * 24\n",
    "pflops_per_gpu = 0.00429 #4.29 Tflops for k40 https://www.nvidia.com/content/tesla/pdf/nvidia-tesla-kepler-family-datasheet.pdf\n",
    "gflop_watts = 18.25 # k40\n",
    "print_stats(\"Zoph and Lee (2016)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=False)\n",
    "\n",
    "# Zoph et al. (2018)\n",
    "# 500 GPUs across 4 days resulting in 2,000 GPU-hours. NVidia P100s.\n",
    "gpu_hours = 2000 #500 * 4 * 24\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "print_stats(\"Zoph et al (2017)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=False)\n",
    "\n",
    "# Zhong et al. (2018)\n",
    "# only spends\n",
    "# 3\n",
    "# days with\n",
    "# 32\n",
    "# GPUs,\n",
    "gpu_hours = 32 * 3 * 24\n",
    "pflops_per_gpu = 0.0106 # 1080ti\n",
    "gflop_watts = 45 # 1080ti\n",
    "print_stats(\"Zhong et al. (2018)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "\n",
    "\n",
    "## Elsken et al.2018\n",
    "# https://arxiv.org/pdf/1804.09081.pdf#page=14&zoom=auto,-139,431\n",
    "# 56 GPU days \n",
    "# Titan X\n",
    "gpu_hours = 56 * 24\n",
    "pflops_per_gpu = 0.01079 # x\n",
    "gflop_watts = 41 # x\n",
    "print_stats(\"Elsken et al. (2018)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "## Elf opengo et al.2018\n",
    "# 2000 GPUs for \n",
    "# assuming p100, not sure\n",
    "gpu_hours = 2000 * 14 * 24\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "print_stats(\"OpenGo et al. (2018)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "\n",
    "## Chen et al.\n",
    "# 370 for 1 week\n",
    "# assume p100\n",
    "gpu_hours = 370 * 7 * 24\n",
    "pflops_per_gpu = 0.0093 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 37.2 # p100\n",
    "print_stats(\"Chen et al. (2018)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n",
    "\n",
    "## OpenAI 1v1 Dota\n",
    "# 256 K80 GPUs for 18 days (from charts)\n",
    "gpu_hours = 256 * 18\n",
    "pflops_per_gpu = 0.0087 # P100 https://en.wikipedia.org/wiki/Nvidia_Tesla\n",
    "gflop_watts = 29.12 # p100\n",
    "print_stats(\"OpenAI 1v1 Dota (2018)\", gpu_hours, pflops_per_gpu, gflop_watts, utilization_constant_use=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
